{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms #give access to popular datasets,arch,and image transformations for CV\n",
    "import  torchvision.datasets as dsets \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset=dsets.MNIST(root='./data', #saves the data in current working base folder and download data in this data folder\n",
    "                          train=True,  #this means it is the training dataset\n",
    "                          transform=transforms.ToTensor(), #remember in pytorch we deal with tensors,so converting dataset to tensor\n",
    "                          download=True) #must when downloading for first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset) #this is a list becoz we can do len only on list,there are 60000 different digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]), 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0] #looking thru this dataset,first element in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset[0]) #its a tuple with first index having image matrix and second as the label of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0] #accessing 1st(0) element of 0 index tuple in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label\n",
    "train_dataset[0][1] #accessing 2nd[1] element of 0 index tuple in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input matrix\n",
    "train_dataset[0][0].size() #getting tensor size of 1st element of 0 index tuple\n",
    "#its basically our image of size 28*28 with channel=1 since its grayscale image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see the image \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img=train_dataset[0][0].numpy().reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb4c5630>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(show_img,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label\n",
    "train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step1b-Loading MNIST Test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=dsets.MNIST(root='./data' ,\n",
    "                         train=False, #putting train=false inorder to get work on test dataset\n",
    "                         transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_dataset[0]) #everything is as per the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2-Make Dataset Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just make the dataset iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total data: 60000\n",
    "\n",
    "minibatch:100\n",
    "\n",
    "Lets say we will do 5 Epochs (1 Epoch:Running through whole dataset once)\n",
    "\n",
    "Iterations=(total data/minibatch)* No. of Epochs=3000 (1 iteration:1 mini-batch forward and backward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5\n",
    "n_iters=(len(train_dataset)/batch_size)*epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Iterable Object:training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True) #sequence changes from epoch to epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Iterability\n",
    "import collections\n",
    "isinstance(train_loader,collections.Iterable) #if something is not iterable it returns false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Iterable Object:training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False) #we keep it false here since we will do only one forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Iterability\n",
    "import collections\n",
    "isinstance(test_loader,collections.Iterable) #if something is not iterable it returns false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3-Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as linear regression since first step in logistic regression is linear regression\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "#     input_dim is x and output_dim is y\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(LogisticRegressionModel,self).__init__()\n",
    "        self.Linear=nn.Linear(input_dim,output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.Linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4-Instantiate Model CLass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].size()\n",
    "# flatten this one to 784 so that our model can interpret it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=28*28 \n",
    "output_dim=10\n",
    "model=LogisticRegressionModel(input_dim,output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5-Instantiate a loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Logistic Regression we use Cross Entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss() #Cross Entropy automatically computes probability alongwith cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6-Instantiate Optimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=.001\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prameter in Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Linear Regression it was simple 1D parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000000000AE5F408>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "print(len(list(model.parameters()))) # it returns 2 becoz pytorch considers bias automatically along with other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Wx+B\n",
    "#(10,784)*(784*1)+(10,1)=(10,1)<--This is the output \n",
    "#Parameters:W\n",
    "print(list(model.parameters())[0].size())\n",
    "#bias parameter:B\n",
    "print(list(model.parameters())[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7-Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 1. Loss 1.0617908239364624. Acuracy 82.\n",
      "Iterations 2. Loss 1.0258756875991821. Acuracy 82.\n",
      "Iterations 3. Loss 1.0450135469436646. Acuracy 82.\n",
      "Iterations 4. Loss 1.0435837507247925. Acuracy 82.\n",
      "Iterations 5. Loss 1.0432329177856445. Acuracy 82.\n",
      "Iterations 6. Loss 1.0664817094802856. Acuracy 82.\n",
      "Iterations 7. Loss 1.072699785232544. Acuracy 82.\n",
      "Iterations 8. Loss 1.024956226348877. Acuracy 82.\n",
      "Iterations 9. Loss 1.1250548362731934. Acuracy 82.\n",
      "Iterations 10. Loss 1.0495288372039795. Acuracy 82.\n",
      "Iterations 11. Loss 1.1103675365447998. Acuracy 82.\n",
      "Iterations 12. Loss 0.9997220039367676. Acuracy 82.\n",
      "Iterations 13. Loss 0.9689196944236755. Acuracy 82.\n",
      "Iterations 14. Loss 1.1095552444458008. Acuracy 82.\n",
      "Iterations 15. Loss 0.9758626818656921. Acuracy 82.\n",
      "Iterations 16. Loss 1.0584901571273804. Acuracy 82.\n",
      "Iterations 17. Loss 1.0263999700546265. Acuracy 82.\n",
      "Iterations 18. Loss 0.9914624094963074. Acuracy 82.\n",
      "Iterations 19. Loss 1.1492242813110352. Acuracy 82.\n",
      "Iterations 20. Loss 1.0018426179885864. Acuracy 82.\n",
      "Iterations 21. Loss 0.9497718214988708. Acuracy 82.\n",
      "Iterations 22. Loss 1.0133771896362305. Acuracy 82.\n",
      "Iterations 23. Loss 1.040450096130371. Acuracy 82.\n",
      "Iterations 24. Loss 1.0501283407211304. Acuracy 82.\n",
      "Iterations 25. Loss 0.9893829226493835. Acuracy 82.\n",
      "Iterations 26. Loss 0.9899948239326477. Acuracy 82.\n",
      "Iterations 27. Loss 1.0803462266921997. Acuracy 82.\n",
      "Iterations 28. Loss 1.0701477527618408. Acuracy 82.\n",
      "Iterations 29. Loss 1.075137972831726. Acuracy 82.\n",
      "Iterations 30. Loss 1.0088423490524292. Acuracy 82.\n",
      "Iterations 31. Loss 0.9934531450271606. Acuracy 82.\n",
      "Iterations 32. Loss 1.0075753927230835. Acuracy 82.\n",
      "Iterations 33. Loss 0.9470751285552979. Acuracy 82.\n",
      "Iterations 34. Loss 0.987084150314331. Acuracy 82.\n",
      "Iterations 35. Loss 0.9848445057868958. Acuracy 82.\n",
      "Iterations 36. Loss 1.0321686267852783. Acuracy 82.\n",
      "Iterations 37. Loss 0.96259605884552. Acuracy 82.\n",
      "Iterations 38. Loss 0.9838579297065735. Acuracy 82.\n",
      "Iterations 39. Loss 0.9448345899581909. Acuracy 82.\n",
      "Iterations 40. Loss 1.0535348653793335. Acuracy 82.\n",
      "Iterations 41. Loss 0.9372609853744507. Acuracy 82.\n",
      "Iterations 42. Loss 1.0025928020477295. Acuracy 82.\n",
      "Iterations 43. Loss 0.9454062581062317. Acuracy 82.\n",
      "Iterations 44. Loss 1.0171740055084229. Acuracy 82.\n",
      "Iterations 45. Loss 0.9916219115257263. Acuracy 82.\n",
      "Iterations 46. Loss 0.9860302805900574. Acuracy 82.\n",
      "Iterations 47. Loss 1.080146312713623. Acuracy 82.\n",
      "Iterations 48. Loss 0.9719737768173218. Acuracy 82.\n",
      "Iterations 49. Loss 1.000167965888977. Acuracy 82.\n",
      "Iterations 50. Loss 1.0515577793121338. Acuracy 82.\n",
      "Iterations 51. Loss 0.986948549747467. Acuracy 82.\n",
      "Iterations 52. Loss 0.9964064955711365. Acuracy 82.\n",
      "Iterations 53. Loss 1.0900095701217651. Acuracy 82.\n",
      "Iterations 54. Loss 1.1233758926391602. Acuracy 82.\n",
      "Iterations 55. Loss 1.0372567176818848. Acuracy 82.\n",
      "Iterations 56. Loss 0.9653443098068237. Acuracy 82.\n",
      "Iterations 57. Loss 1.04042387008667. Acuracy 82.\n",
      "Iterations 58. Loss 0.9798344969749451. Acuracy 82.\n",
      "Iterations 59. Loss 0.9607452154159546. Acuracy 82.\n",
      "Iterations 60. Loss 1.1348003149032593. Acuracy 82.\n",
      "Iterations 61. Loss 1.0701844692230225. Acuracy 82.\n",
      "Iterations 62. Loss 1.0410960912704468. Acuracy 82.\n",
      "Iterations 63. Loss 1.0575227737426758. Acuracy 82.\n",
      "Iterations 64. Loss 1.0417780876159668. Acuracy 82.\n",
      "Iterations 65. Loss 0.9629181623458862. Acuracy 82.\n",
      "Iterations 66. Loss 1.0813194513320923. Acuracy 82.\n",
      "Iterations 67. Loss 0.9744042158126831. Acuracy 82.\n",
      "Iterations 68. Loss 1.1062904596328735. Acuracy 82.\n",
      "Iterations 69. Loss 1.094948649406433. Acuracy 82.\n",
      "Iterations 70. Loss 1.0578058958053589. Acuracy 82.\n",
      "Iterations 71. Loss 1.0126063823699951. Acuracy 82.\n",
      "Iterations 72. Loss 0.9969190955162048. Acuracy 82.\n",
      "Iterations 73. Loss 1.1414507627487183. Acuracy 82.\n",
      "Iterations 74. Loss 1.1170759201049805. Acuracy 82.\n",
      "Iterations 75. Loss 1.12827730178833. Acuracy 82.\n",
      "Iterations 76. Loss 1.0280036926269531. Acuracy 82.\n",
      "Iterations 77. Loss 0.9944446086883545. Acuracy 82.\n",
      "Iterations 78. Loss 1.091545820236206. Acuracy 82.\n",
      "Iterations 79. Loss 1.0294785499572754. Acuracy 82.\n",
      "Iterations 80. Loss 0.9797832369804382. Acuracy 82.\n",
      "Iterations 81. Loss 1.0678412914276123. Acuracy 82.\n",
      "Iterations 82. Loss 1.0459740161895752. Acuracy 82.\n",
      "Iterations 83. Loss 1.1236335039138794. Acuracy 82.\n",
      "Iterations 84. Loss 1.1356128454208374. Acuracy 82.\n",
      "Iterations 85. Loss 1.0408358573913574. Acuracy 82.\n",
      "Iterations 86. Loss 1.0529555082321167. Acuracy 82.\n",
      "Iterations 87. Loss 1.0085123777389526. Acuracy 82.\n",
      "Iterations 88. Loss 1.0871222019195557. Acuracy 82.\n",
      "Iterations 89. Loss 1.0736021995544434. Acuracy 82.\n",
      "Iterations 90. Loss 1.0362147092819214. Acuracy 82.\n",
      "Iterations 91. Loss 0.9526512026786804. Acuracy 82.\n",
      "Iterations 92. Loss 1.0921428203582764. Acuracy 82.\n",
      "Iterations 93. Loss 1.0883171558380127. Acuracy 82.\n",
      "Iterations 94. Loss 1.0396876335144043. Acuracy 82.\n",
      "Iterations 95. Loss 1.012073278427124. Acuracy 82.\n",
      "Iterations 96. Loss 1.0647493600845337. Acuracy 82.\n",
      "Iterations 97. Loss 1.082135796546936. Acuracy 82.\n",
      "Iterations 98. Loss 1.0079894065856934. Acuracy 82.\n",
      "Iterations 99. Loss 1.0840561389923096. Acuracy 82.\n",
      "Iterations 100. Loss 1.0674139261245728. Acuracy 82.\n",
      "Iterations 101. Loss 1.0197267532348633. Acuracy 82.\n",
      "Iterations 102. Loss 1.023583173751831. Acuracy 82.\n",
      "Iterations 103. Loss 1.008089303970337. Acuracy 82.\n",
      "Iterations 104. Loss 1.092952013015747. Acuracy 82.\n",
      "Iterations 105. Loss 0.9579576849937439. Acuracy 82.\n",
      "Iterations 106. Loss 1.00456964969635. Acuracy 82.\n",
      "Iterations 107. Loss 1.1204833984375. Acuracy 82.\n",
      "Iterations 108. Loss 1.1070623397827148. Acuracy 82.\n",
      "Iterations 109. Loss 0.9791290760040283. Acuracy 82.\n",
      "Iterations 110. Loss 1.0749597549438477. Acuracy 82.\n",
      "Iterations 111. Loss 1.0882402658462524. Acuracy 82.\n",
      "Iterations 112. Loss 0.8551818132400513. Acuracy 82.\n",
      "Iterations 113. Loss 1.0197265148162842. Acuracy 82.\n",
      "Iterations 114. Loss 1.0243217945098877. Acuracy 82.\n",
      "Iterations 115. Loss 1.0468075275421143. Acuracy 82.\n",
      "Iterations 116. Loss 1.0434130430221558. Acuracy 82.\n",
      "Iterations 117. Loss 1.1175438165664673. Acuracy 82.\n",
      "Iterations 118. Loss 0.9738069176673889. Acuracy 82.\n",
      "Iterations 119. Loss 1.054171085357666. Acuracy 82.\n",
      "Iterations 120. Loss 1.0252275466918945. Acuracy 82.\n",
      "Iterations 121. Loss 1.0206321477890015. Acuracy 82.\n",
      "Iterations 122. Loss 0.9569647908210754. Acuracy 82.\n",
      "Iterations 123. Loss 1.0126150846481323. Acuracy 82.\n",
      "Iterations 124. Loss 1.1078277826309204. Acuracy 82.\n",
      "Iterations 125. Loss 0.9808303117752075. Acuracy 82.\n",
      "Iterations 126. Loss 1.0518745183944702. Acuracy 82.\n",
      "Iterations 127. Loss 1.0204623937606812. Acuracy 82.\n",
      "Iterations 128. Loss 1.1081868410110474. Acuracy 82.\n",
      "Iterations 129. Loss 1.1275293827056885. Acuracy 82.\n",
      "Iterations 130. Loss 0.9594483971595764. Acuracy 82.\n",
      "Iterations 131. Loss 0.9853360652923584. Acuracy 82.\n",
      "Iterations 132. Loss 0.9261082410812378. Acuracy 82.\n",
      "Iterations 133. Loss 1.0145392417907715. Acuracy 82.\n",
      "Iterations 134. Loss 1.0294508934020996. Acuracy 82.\n",
      "Iterations 135. Loss 1.0046190023422241. Acuracy 82.\n",
      "Iterations 136. Loss 0.9899805188179016. Acuracy 82.\n",
      "Iterations 137. Loss 1.1519252061843872. Acuracy 82.\n",
      "Iterations 138. Loss 1.0512663125991821. Acuracy 82.\n",
      "Iterations 139. Loss 1.0072766542434692. Acuracy 82.\n",
      "Iterations 140. Loss 0.9786475896835327. Acuracy 82.\n",
      "Iterations 141. Loss 1.0413296222686768. Acuracy 82.\n",
      "Iterations 142. Loss 1.0215340852737427. Acuracy 82.\n",
      "Iterations 143. Loss 1.0177018642425537. Acuracy 82.\n",
      "Iterations 144. Loss 1.1004964113235474. Acuracy 82.\n",
      "Iterations 145. Loss 1.0279669761657715. Acuracy 82.\n",
      "Iterations 146. Loss 1.0181379318237305. Acuracy 82.\n",
      "Iterations 147. Loss 1.1266762018203735. Acuracy 82.\n",
      "Iterations 148. Loss 0.9463550448417664. Acuracy 82.\n",
      "Iterations 149. Loss 1.0580753087997437. Acuracy 82.\n",
      "Iterations 150. Loss 1.0695880651474. Acuracy 82.\n",
      "Iterations 151. Loss 0.9431473016738892. Acuracy 82.\n",
      "Iterations 152. Loss 0.9930828213691711. Acuracy 82.\n",
      "Iterations 153. Loss 0.9548388123512268. Acuracy 82.\n",
      "Iterations 154. Loss 0.9602697491645813. Acuracy 82.\n",
      "Iterations 155. Loss 1.0665348768234253. Acuracy 82.\n",
      "Iterations 156. Loss 1.0237095355987549. Acuracy 82.\n",
      "Iterations 157. Loss 0.9537280201911926. Acuracy 82.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 158. Loss 0.9583495855331421. Acuracy 82.\n",
      "Iterations 159. Loss 1.0272037982940674. Acuracy 82.\n",
      "Iterations 160. Loss 1.015979528427124. Acuracy 82.\n",
      "Iterations 161. Loss 1.0147327184677124. Acuracy 82.\n",
      "Iterations 162. Loss 1.0187156200408936. Acuracy 82.\n",
      "Iterations 163. Loss 0.9622434973716736. Acuracy 82.\n",
      "Iterations 164. Loss 1.014620065689087. Acuracy 82.\n",
      "Iterations 165. Loss 1.030771255493164. Acuracy 82.\n",
      "Iterations 166. Loss 0.9945569038391113. Acuracy 82.\n",
      "Iterations 167. Loss 1.0493732690811157. Acuracy 82.\n",
      "Iterations 168. Loss 1.0154112577438354. Acuracy 82.\n",
      "Iterations 169. Loss 1.147772192955017. Acuracy 82.\n",
      "Iterations 170. Loss 0.9469773173332214. Acuracy 82.\n",
      "Iterations 171. Loss 0.9960536360740662. Acuracy 82.\n",
      "Iterations 172. Loss 1.0477938652038574. Acuracy 82.\n",
      "Iterations 173. Loss 0.9822373986244202. Acuracy 82.\n",
      "Iterations 174. Loss 1.0643303394317627. Acuracy 82.\n",
      "Iterations 175. Loss 1.078066349029541. Acuracy 82.\n",
      "Iterations 176. Loss 1.0431824922561646. Acuracy 82.\n",
      "Iterations 177. Loss 0.9594219923019409. Acuracy 82.\n",
      "Iterations 178. Loss 1.0397924184799194. Acuracy 82.\n",
      "Iterations 179. Loss 1.0451569557189941. Acuracy 82.\n",
      "Iterations 180. Loss 1.018416404724121. Acuracy 82.\n",
      "Iterations 181. Loss 1.0436207056045532. Acuracy 82.\n",
      "Iterations 182. Loss 0.9561500549316406. Acuracy 82.\n",
      "Iterations 183. Loss 1.0876693725585938. Acuracy 82.\n",
      "Iterations 184. Loss 0.9821754693984985. Acuracy 82.\n",
      "Iterations 185. Loss 0.9711385369300842. Acuracy 82.\n",
      "Iterations 186. Loss 1.0293149948120117. Acuracy 82.\n",
      "Iterations 187. Loss 1.0003618001937866. Acuracy 82.\n",
      "Iterations 188. Loss 0.9695693254470825. Acuracy 82.\n",
      "Iterations 189. Loss 0.9352773427963257. Acuracy 82.\n",
      "Iterations 190. Loss 1.0392638444900513. Acuracy 82.\n",
      "Iterations 191. Loss 0.9779474139213562. Acuracy 82.\n",
      "Iterations 192. Loss 0.9772940874099731. Acuracy 82.\n",
      "Iterations 193. Loss 0.9276161193847656. Acuracy 82.\n",
      "Iterations 194. Loss 0.9551065564155579. Acuracy 82.\n",
      "Iterations 195. Loss 0.9424136877059937. Acuracy 82.\n",
      "Iterations 196. Loss 0.9984409809112549. Acuracy 82.\n",
      "Iterations 197. Loss 1.068617820739746. Acuracy 82.\n",
      "Iterations 198. Loss 0.9418802857398987. Acuracy 82.\n",
      "Iterations 199. Loss 1.001519799232483. Acuracy 82.\n",
      "Iterations 200. Loss 1.0287392139434814. Acuracy 82.\n",
      "Iterations 201. Loss 0.9171929359436035. Acuracy 82.\n",
      "Iterations 202. Loss 1.0338448286056519. Acuracy 82.\n",
      "Iterations 203. Loss 1.0323950052261353. Acuracy 82.\n",
      "Iterations 204. Loss 1.0712485313415527. Acuracy 82.\n",
      "Iterations 205. Loss 1.051459550857544. Acuracy 82.\n",
      "Iterations 206. Loss 1.0142548084259033. Acuracy 82.\n",
      "Iterations 207. Loss 1.0874072313308716. Acuracy 82.\n",
      "Iterations 208. Loss 0.9963951706886292. Acuracy 82.\n",
      "Iterations 209. Loss 1.0098415613174438. Acuracy 82.\n",
      "Iterations 210. Loss 0.9602421522140503. Acuracy 82.\n",
      "Iterations 211. Loss 1.0542086362838745. Acuracy 82.\n",
      "Iterations 212. Loss 1.0182392597198486. Acuracy 82.\n",
      "Iterations 213. Loss 1.0249698162078857. Acuracy 82.\n",
      "Iterations 214. Loss 0.9800933003425598. Acuracy 82.\n",
      "Iterations 215. Loss 1.1077486276626587. Acuracy 82.\n",
      "Iterations 216. Loss 0.9665224552154541. Acuracy 82.\n",
      "Iterations 217. Loss 0.9791790843009949. Acuracy 82.\n",
      "Iterations 218. Loss 0.9780694842338562. Acuracy 82.\n",
      "Iterations 219. Loss 1.0238120555877686. Acuracy 82.\n",
      "Iterations 220. Loss 0.9943168759346008. Acuracy 82.\n",
      "Iterations 221. Loss 1.0731401443481445. Acuracy 82.\n",
      "Iterations 222. Loss 1.0243059396743774. Acuracy 82.\n",
      "Iterations 223. Loss 0.9037496447563171. Acuracy 82.\n",
      "Iterations 224. Loss 1.0079270601272583. Acuracy 82.\n",
      "Iterations 225. Loss 0.9769899845123291. Acuracy 82.\n",
      "Iterations 226. Loss 0.9971751570701599. Acuracy 82.\n",
      "Iterations 227. Loss 1.0476105213165283. Acuracy 82.\n",
      "Iterations 228. Loss 1.0807619094848633. Acuracy 82.\n",
      "Iterations 229. Loss 0.8756458759307861. Acuracy 82.\n",
      "Iterations 230. Loss 1.0664421319961548. Acuracy 82.\n",
      "Iterations 231. Loss 0.9581862092018127. Acuracy 82.\n",
      "Iterations 232. Loss 0.9542020559310913. Acuracy 82.\n",
      "Iterations 233. Loss 0.9333839416503906. Acuracy 82.\n",
      "Iterations 234. Loss 1.0501031875610352. Acuracy 82.\n",
      "Iterations 235. Loss 1.0217868089675903. Acuracy 82.\n",
      "Iterations 236. Loss 1.0194764137268066. Acuracy 82.\n",
      "Iterations 237. Loss 1.0867217779159546. Acuracy 82.\n",
      "Iterations 238. Loss 1.023813247680664. Acuracy 82.\n",
      "Iterations 239. Loss 0.9338017106056213. Acuracy 82.\n",
      "Iterations 240. Loss 0.9388857483863831. Acuracy 82.\n",
      "Iterations 241. Loss 1.0333176851272583. Acuracy 82.\n",
      "Iterations 242. Loss 1.0338472127914429. Acuracy 82.\n",
      "Iterations 243. Loss 0.940625011920929. Acuracy 82.\n",
      "Iterations 244. Loss 1.1112571954727173. Acuracy 82.\n",
      "Iterations 245. Loss 1.0464733839035034. Acuracy 82.\n",
      "Iterations 246. Loss 0.9975290894508362. Acuracy 82.\n",
      "Iterations 247. Loss 0.9422075748443604. Acuracy 82.\n",
      "Iterations 248. Loss 0.971055805683136. Acuracy 82.\n",
      "Iterations 249. Loss 0.938191831111908. Acuracy 82.\n",
      "Iterations 250. Loss 0.9871965646743774. Acuracy 82.\n",
      "Iterations 251. Loss 0.8619964718818665. Acuracy 82.\n",
      "Iterations 252. Loss 0.9675647020339966. Acuracy 82.\n",
      "Iterations 253. Loss 1.027235507965088. Acuracy 82.\n",
      "Iterations 254. Loss 1.0034515857696533. Acuracy 82.\n",
      "Iterations 255. Loss 0.9711295366287231. Acuracy 82.\n",
      "Iterations 256. Loss 0.951468825340271. Acuracy 82.\n",
      "Iterations 257. Loss 1.0992252826690674. Acuracy 82.\n",
      "Iterations 258. Loss 1.0373828411102295. Acuracy 82.\n",
      "Iterations 259. Loss 1.044886589050293. Acuracy 82.\n",
      "Iterations 260. Loss 0.9873499274253845. Acuracy 82.\n",
      "Iterations 261. Loss 0.9571028351783752. Acuracy 82.\n",
      "Iterations 262. Loss 0.9101724028587341. Acuracy 82.\n",
      "Iterations 263. Loss 0.9927304983139038. Acuracy 82.\n",
      "Iterations 264. Loss 1.0967057943344116. Acuracy 82.\n",
      "Iterations 265. Loss 0.9313142895698547. Acuracy 82.\n",
      "Iterations 266. Loss 1.0816105604171753. Acuracy 82.\n",
      "Iterations 267. Loss 1.0204561948776245. Acuracy 82.\n",
      "Iterations 268. Loss 0.8887042999267578. Acuracy 82.\n",
      "Iterations 269. Loss 0.9374480247497559. Acuracy 82.\n",
      "Iterations 270. Loss 0.9940907955169678. Acuracy 82.\n",
      "Iterations 271. Loss 1.001731038093567. Acuracy 82.\n",
      "Iterations 272. Loss 0.9027464389801025. Acuracy 82.\n",
      "Iterations 273. Loss 0.9692041873931885. Acuracy 82.\n",
      "Iterations 274. Loss 1.0183371305465698. Acuracy 82.\n",
      "Iterations 275. Loss 1.048134684562683. Acuracy 82.\n",
      "Iterations 276. Loss 0.9474475383758545. Acuracy 82.\n",
      "Iterations 277. Loss 1.0318939685821533. Acuracy 82.\n",
      "Iterations 278. Loss 1.0258740186691284. Acuracy 82.\n",
      "Iterations 279. Loss 1.0916380882263184. Acuracy 82.\n",
      "Iterations 280. Loss 0.9775170683860779. Acuracy 82.\n",
      "Iterations 281. Loss 0.8968372344970703. Acuracy 82.\n",
      "Iterations 282. Loss 1.0275278091430664. Acuracy 82.\n",
      "Iterations 283. Loss 1.0151982307434082. Acuracy 82.\n",
      "Iterations 284. Loss 0.9607418179512024. Acuracy 82.\n",
      "Iterations 285. Loss 0.9618207812309265. Acuracy 82.\n",
      "Iterations 286. Loss 0.9035502076148987. Acuracy 82.\n",
      "Iterations 287. Loss 0.9863674640655518. Acuracy 82.\n",
      "Iterations 288. Loss 0.9960551261901855. Acuracy 82.\n",
      "Iterations 289. Loss 1.0118929147720337. Acuracy 82.\n",
      "Iterations 290. Loss 1.0103968381881714. Acuracy 82.\n",
      "Iterations 291. Loss 1.0128706693649292. Acuracy 82.\n",
      "Iterations 292. Loss 1.067915916442871. Acuracy 82.\n",
      "Iterations 293. Loss 0.9096012115478516. Acuracy 82.\n",
      "Iterations 294. Loss 1.0924577713012695. Acuracy 82.\n",
      "Iterations 295. Loss 0.951566219329834. Acuracy 82.\n",
      "Iterations 296. Loss 1.0575528144836426. Acuracy 82.\n",
      "Iterations 297. Loss 1.018391728401184. Acuracy 82.\n",
      "Iterations 298. Loss 1.0010262727737427. Acuracy 82.\n",
      "Iterations 299. Loss 1.0795395374298096. Acuracy 82.\n",
      "Iterations 300. Loss 0.9987727403640747. Acuracy 82.\n",
      "Iterations 301. Loss 0.9527340531349182. Acuracy 82.\n",
      "Iterations 302. Loss 0.9478563666343689. Acuracy 82.\n",
      "Iterations 303. Loss 0.9512114524841309. Acuracy 82.\n",
      "Iterations 304. Loss 1.008193850517273. Acuracy 82.\n",
      "Iterations 305. Loss 0.9998893737792969. Acuracy 82.\n",
      "Iterations 306. Loss 0.970401406288147. Acuracy 82.\n",
      "Iterations 307. Loss 1.0147875547409058. Acuracy 82.\n",
      "Iterations 308. Loss 0.9496473670005798. Acuracy 82.\n",
      "Iterations 309. Loss 1.0960571765899658. Acuracy 82.\n",
      "Iterations 310. Loss 0.9494069814682007. Acuracy 82.\n",
      "Iterations 311. Loss 1.0703274011611938. Acuracy 82.\n",
      "Iterations 312. Loss 0.9106529355049133. Acuracy 82.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 313. Loss 1.0633352994918823. Acuracy 82.\n",
      "Iterations 314. Loss 0.9766094088554382. Acuracy 82.\n",
      "Iterations 315. Loss 0.9896830916404724. Acuracy 82.\n",
      "Iterations 316. Loss 0.9719609618186951. Acuracy 82.\n",
      "Iterations 317. Loss 0.9734331369400024. Acuracy 82.\n",
      "Iterations 318. Loss 1.0403648614883423. Acuracy 82.\n",
      "Iterations 319. Loss 0.9983232021331787. Acuracy 82.\n",
      "Iterations 320. Loss 1.026971459388733. Acuracy 82.\n",
      "Iterations 321. Loss 0.9470558762550354. Acuracy 82.\n",
      "Iterations 322. Loss 0.9319401383399963. Acuracy 82.\n",
      "Iterations 323. Loss 1.0631147623062134. Acuracy 82.\n",
      "Iterations 324. Loss 1.023168683052063. Acuracy 82.\n",
      "Iterations 325. Loss 0.9338685870170593. Acuracy 82.\n",
      "Iterations 326. Loss 1.0877035856246948. Acuracy 82.\n",
      "Iterations 327. Loss 1.008986473083496. Acuracy 82.\n",
      "Iterations 328. Loss 1.000103235244751. Acuracy 82.\n",
      "Iterations 329. Loss 0.9771014451980591. Acuracy 82.\n",
      "Iterations 330. Loss 1.0422736406326294. Acuracy 82.\n",
      "Iterations 331. Loss 0.9875748157501221. Acuracy 82.\n",
      "Iterations 332. Loss 1.006629228591919. Acuracy 82.\n",
      "Iterations 333. Loss 1.089622974395752. Acuracy 82.\n",
      "Iterations 334. Loss 1.0347012281417847. Acuracy 82.\n",
      "Iterations 335. Loss 0.9385784268379211. Acuracy 82.\n",
      "Iterations 336. Loss 0.9960340261459351. Acuracy 82.\n",
      "Iterations 337. Loss 1.0966905355453491. Acuracy 82.\n",
      "Iterations 338. Loss 1.0372580289840698. Acuracy 82.\n",
      "Iterations 339. Loss 0.9813240766525269. Acuracy 82.\n",
      "Iterations 340. Loss 0.9713281989097595. Acuracy 82.\n",
      "Iterations 341. Loss 0.9414640069007874. Acuracy 82.\n",
      "Iterations 342. Loss 1.020725131034851. Acuracy 82.\n",
      "Iterations 343. Loss 0.960278332233429. Acuracy 82.\n",
      "Iterations 344. Loss 0.9530488848686218. Acuracy 82.\n",
      "Iterations 345. Loss 0.9296533465385437. Acuracy 82.\n",
      "Iterations 346. Loss 0.8871929049491882. Acuracy 82.\n",
      "Iterations 347. Loss 1.0272483825683594. Acuracy 82.\n",
      "Iterations 348. Loss 0.9889633059501648. Acuracy 82.\n",
      "Iterations 349. Loss 0.8507686853408813. Acuracy 82.\n",
      "Iterations 350. Loss 1.0127476453781128. Acuracy 82.\n",
      "Iterations 351. Loss 1.0458749532699585. Acuracy 82.\n",
      "Iterations 352. Loss 0.9115976691246033. Acuracy 82.\n",
      "Iterations 353. Loss 1.0230463743209839. Acuracy 82.\n",
      "Iterations 354. Loss 1.0051498413085938. Acuracy 82.\n",
      "Iterations 355. Loss 0.968936562538147. Acuracy 82.\n",
      "Iterations 356. Loss 0.9567442536354065. Acuracy 82.\n",
      "Iterations 357. Loss 0.9694766402244568. Acuracy 82.\n",
      "Iterations 358. Loss 1.0487169027328491. Acuracy 82.\n",
      "Iterations 359. Loss 1.1071261167526245. Acuracy 82.\n",
      "Iterations 360. Loss 0.9822099208831787. Acuracy 82.\n",
      "Iterations 361. Loss 0.9858705997467041. Acuracy 82.\n",
      "Iterations 362. Loss 1.0059162378311157. Acuracy 82.\n",
      "Iterations 363. Loss 0.9516468644142151. Acuracy 82.\n",
      "Iterations 364. Loss 0.9485822916030884. Acuracy 82.\n",
      "Iterations 365. Loss 0.945762038230896. Acuracy 82.\n",
      "Iterations 366. Loss 0.9474642276763916. Acuracy 82.\n",
      "Iterations 367. Loss 0.9902973771095276. Acuracy 82.\n",
      "Iterations 368. Loss 1.0038982629776. Acuracy 82.\n",
      "Iterations 369. Loss 1.0649522542953491. Acuracy 82.\n",
      "Iterations 370. Loss 0.922806978225708. Acuracy 82.\n",
      "Iterations 371. Loss 0.9971969723701477. Acuracy 82.\n",
      "Iterations 372. Loss 1.030290126800537. Acuracy 82.\n",
      "Iterations 373. Loss 0.9654733538627625. Acuracy 82.\n",
      "Iterations 374. Loss 0.9253389239311218. Acuracy 82.\n",
      "Iterations 375. Loss 0.9667061567306519. Acuracy 82.\n",
      "Iterations 376. Loss 0.9765269756317139. Acuracy 82.\n",
      "Iterations 377. Loss 0.9278253316879272. Acuracy 82.\n",
      "Iterations 378. Loss 1.0030207633972168. Acuracy 82.\n",
      "Iterations 379. Loss 0.9159064292907715. Acuracy 82.\n",
      "Iterations 380. Loss 0.8890542387962341. Acuracy 82.\n",
      "Iterations 381. Loss 0.8635089993476868. Acuracy 82.\n",
      "Iterations 382. Loss 0.9317329525947571. Acuracy 82.\n",
      "Iterations 383. Loss 1.0617642402648926. Acuracy 82.\n",
      "Iterations 384. Loss 0.9450640082359314. Acuracy 82.\n",
      "Iterations 385. Loss 0.9133058786392212. Acuracy 82.\n",
      "Iterations 386. Loss 0.9360370635986328. Acuracy 82.\n",
      "Iterations 387. Loss 0.9561220407485962. Acuracy 82.\n",
      "Iterations 388. Loss 1.0275455713272095. Acuracy 82.\n",
      "Iterations 389. Loss 0.9949434399604797. Acuracy 82.\n",
      "Iterations 390. Loss 0.952434778213501. Acuracy 82.\n",
      "Iterations 391. Loss 0.9959924817085266. Acuracy 82.\n",
      "Iterations 392. Loss 0.9371642470359802. Acuracy 82.\n",
      "Iterations 393. Loss 0.9659995436668396. Acuracy 82.\n",
      "Iterations 394. Loss 0.9764159321784973. Acuracy 82.\n",
      "Iterations 395. Loss 1.0019049644470215. Acuracy 82.\n",
      "Iterations 396. Loss 1.024299144744873. Acuracy 82.\n",
      "Iterations 397. Loss 0.8817802667617798. Acuracy 82.\n",
      "Iterations 398. Loss 1.0119856595993042. Acuracy 82.\n",
      "Iterations 399. Loss 1.0135548114776611. Acuracy 82.\n",
      "Iterations 400. Loss 1.0015391111373901. Acuracy 82.\n",
      "Iterations 401. Loss 0.9790495038032532. Acuracy 82.\n",
      "Iterations 402. Loss 1.1396024227142334. Acuracy 82.\n",
      "Iterations 403. Loss 0.9554715156555176. Acuracy 82.\n",
      "Iterations 404. Loss 0.8856459259986877. Acuracy 82.\n",
      "Iterations 405. Loss 0.9258852601051331. Acuracy 82.\n",
      "Iterations 406. Loss 0.952639639377594. Acuracy 82.\n",
      "Iterations 407. Loss 1.0250462293624878. Acuracy 82.\n",
      "Iterations 408. Loss 0.9797308444976807. Acuracy 82.\n",
      "Iterations 409. Loss 0.9056838154792786. Acuracy 82.\n",
      "Iterations 410. Loss 1.0074098110198975. Acuracy 82.\n",
      "Iterations 411. Loss 0.9391977787017822. Acuracy 82.\n",
      "Iterations 412. Loss 0.9479907751083374. Acuracy 82.\n",
      "Iterations 413. Loss 0.9575071930885315. Acuracy 82.\n",
      "Iterations 414. Loss 1.0684702396392822. Acuracy 82.\n",
      "Iterations 415. Loss 0.9887611269950867. Acuracy 82.\n",
      "Iterations 416. Loss 1.0647227764129639. Acuracy 82.\n",
      "Iterations 417. Loss 0.9929209351539612. Acuracy 82.\n",
      "Iterations 418. Loss 1.0373042821884155. Acuracy 82.\n",
      "Iterations 419. Loss 1.0196435451507568. Acuracy 82.\n",
      "Iterations 420. Loss 0.9750517010688782. Acuracy 82.\n",
      "Iterations 421. Loss 1.0125218629837036. Acuracy 82.\n",
      "Iterations 422. Loss 1.085186243057251. Acuracy 82.\n",
      "Iterations 423. Loss 0.8681238293647766. Acuracy 82.\n",
      "Iterations 424. Loss 0.8877894878387451. Acuracy 82.\n",
      "Iterations 425. Loss 1.055015206336975. Acuracy 82.\n",
      "Iterations 426. Loss 0.9958568811416626. Acuracy 82.\n",
      "Iterations 427. Loss 0.8747001886367798. Acuracy 82.\n",
      "Iterations 428. Loss 0.9421106576919556. Acuracy 82.\n",
      "Iterations 429. Loss 0.9448153972625732. Acuracy 82.\n",
      "Iterations 430. Loss 1.058910846710205. Acuracy 82.\n",
      "Iterations 431. Loss 0.8855321407318115. Acuracy 82.\n",
      "Iterations 432. Loss 0.799933910369873. Acuracy 82.\n",
      "Iterations 433. Loss 0.9586331844329834. Acuracy 82.\n",
      "Iterations 434. Loss 0.9560688138008118. Acuracy 82.\n",
      "Iterations 435. Loss 0.9444188475608826. Acuracy 82.\n",
      "Iterations 436. Loss 0.8873896598815918. Acuracy 82.\n",
      "Iterations 437. Loss 0.8989527225494385. Acuracy 82.\n",
      "Iterations 438. Loss 0.985344648361206. Acuracy 82.\n",
      "Iterations 439. Loss 0.9308640956878662. Acuracy 82.\n",
      "Iterations 440. Loss 0.9931299090385437. Acuracy 82.\n",
      "Iterations 441. Loss 0.9402130842208862. Acuracy 82.\n",
      "Iterations 442. Loss 0.9292458891868591. Acuracy 82.\n",
      "Iterations 443. Loss 0.9614524245262146. Acuracy 82.\n",
      "Iterations 444. Loss 0.8771819472312927. Acuracy 82.\n",
      "Iterations 445. Loss 1.0332759618759155. Acuracy 82.\n",
      "Iterations 446. Loss 0.9572497010231018. Acuracy 82.\n",
      "Iterations 447. Loss 0.9967808723449707. Acuracy 82.\n",
      "Iterations 448. Loss 0.8659625053405762. Acuracy 82.\n",
      "Iterations 449. Loss 1.0476300716400146. Acuracy 82.\n",
      "Iterations 450. Loss 0.8177063465118408. Acuracy 82.\n",
      "Iterations 451. Loss 1.0110294818878174. Acuracy 82.\n",
      "Iterations 452. Loss 0.942827045917511. Acuracy 82.\n",
      "Iterations 453. Loss 0.9511333703994751. Acuracy 82.\n",
      "Iterations 454. Loss 1.0573502779006958. Acuracy 82.\n",
      "Iterations 455. Loss 1.0112782716751099. Acuracy 82.\n",
      "Iterations 456. Loss 1.0420070886611938. Acuracy 82.\n",
      "Iterations 457. Loss 0.9415351152420044. Acuracy 82.\n",
      "Iterations 458. Loss 0.9214613437652588. Acuracy 82.\n",
      "Iterations 459. Loss 0.9988501667976379. Acuracy 82.\n",
      "Iterations 460. Loss 0.9883760809898376. Acuracy 82.\n",
      "Iterations 461. Loss 0.9582124352455139. Acuracy 82.\n",
      "Iterations 462. Loss 1.0181721448898315. Acuracy 82.\n",
      "Iterations 463. Loss 0.9907354712486267. Acuracy 82.\n",
      "Iterations 464. Loss 0.9615917801856995. Acuracy 82.\n",
      "Iterations 465. Loss 0.9543036818504333. Acuracy 82.\n",
      "Iterations 466. Loss 1.0452120304107666. Acuracy 82.\n",
      "Iterations 467. Loss 1.0290428400039673. Acuracy 82.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 468. Loss 0.9763523936271667. Acuracy 82.\n",
      "Iterations 469. Loss 1.0479680299758911. Acuracy 82.\n",
      "Iterations 470. Loss 0.9630277156829834. Acuracy 82.\n",
      "Iterations 471. Loss 0.9238534569740295. Acuracy 82.\n",
      "Iterations 472. Loss 0.8446686267852783. Acuracy 82.\n",
      "Iterations 473. Loss 0.9392833113670349. Acuracy 82.\n",
      "Iterations 474. Loss 0.9125628471374512. Acuracy 82.\n",
      "Iterations 475. Loss 1.0271787643432617. Acuracy 82.\n",
      "Iterations 476. Loss 0.9062348008155823. Acuracy 82.\n",
      "Iterations 477. Loss 1.0157973766326904. Acuracy 82.\n",
      "Iterations 478. Loss 0.9155383110046387. Acuracy 82.\n",
      "Iterations 479. Loss 1.0592103004455566. Acuracy 82.\n",
      "Iterations 480. Loss 0.9640820026397705. Acuracy 82.\n",
      "Iterations 481. Loss 1.025484323501587. Acuracy 82.\n",
      "Iterations 482. Loss 1.0131216049194336. Acuracy 82.\n",
      "Iterations 483. Loss 1.0514096021652222. Acuracy 82.\n",
      "Iterations 484. Loss 0.9248471856117249. Acuracy 82.\n",
      "Iterations 485. Loss 0.9691184759140015. Acuracy 82.\n",
      "Iterations 486. Loss 1.0044416189193726. Acuracy 82.\n",
      "Iterations 487. Loss 0.9282281398773193. Acuracy 82.\n",
      "Iterations 488. Loss 0.9716947078704834. Acuracy 82.\n",
      "Iterations 489. Loss 0.9712333679199219. Acuracy 82.\n",
      "Iterations 490. Loss 0.9317589402198792. Acuracy 82.\n",
      "Iterations 491. Loss 0.9328263998031616. Acuracy 82.\n",
      "Iterations 492. Loss 1.0310808420181274. Acuracy 82.\n",
      "Iterations 493. Loss 0.9634498357772827. Acuracy 82.\n",
      "Iterations 494. Loss 0.9805943965911865. Acuracy 82.\n",
      "Iterations 495. Loss 0.8899587988853455. Acuracy 82.\n",
      "Iterations 496. Loss 0.9849398136138916. Acuracy 82.\n",
      "Iterations 497. Loss 1.021088719367981. Acuracy 82.\n",
      "Iterations 498. Loss 0.928698718547821. Acuracy 82.\n",
      "Iterations 499. Loss 0.8944305181503296. Acuracy 82.\n",
      "4\n",
      "99\n",
      "Iterations 500. Loss 0.9235317707061768. Acuracy 83.\n"
     ]
    }
   ],
   "source": [
    "iter=0\n",
    "for epoch in range(int(epochs)):\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        #load images as Variables\n",
    "        images=Variable(images.view(-1,28*28))\n",
    "#         print(images.size())\n",
    "        labels=Variable(labels)\n",
    "        \n",
    "        #Clear Gradients wrt parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward pass to get output/logits\n",
    "        outputs=model(images)\n",
    "        \n",
    "        #cal loss:Softmax-->cross entropy loss\n",
    "        loss=criterion(outputs,labels)\n",
    "        \n",
    "        #getting gradients wrt parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        #updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter+=1\n",
    "        \n",
    "        if iter%500==0:\n",
    "            #calculate Accuracy\n",
    "            correct=0\n",
    "            total=0\n",
    "            #iterate thru dataset\n",
    "            for images,labels in test_loader:\n",
    "                images=Variable(images.view(-1,28*28))\n",
    "                outputs=model(images)\n",
    "                \n",
    "                _,predicted=torch.max(outputs.data,1)\n",
    "                \n",
    "                total+=labels.size(0)\n",
    "                \n",
    "                correct+=(predicted==labels).sum()\n",
    "            accuracy=100*correct/total\n",
    "            \n",
    "            print(epoch)\n",
    "            print(i)\n",
    "            print('Iterations {}. Loss {}. Acuracy {}.'.format(iter,loss.data[0],accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
